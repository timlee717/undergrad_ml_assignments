{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34e5ae83",
   "metadata": {},
   "source": [
    "# Assignment 1: Wrangling and EDA\n",
    "### Foundations of Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c69ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691fcb3e",
   "metadata": {},
   "source": [
    "**Q1.** This question provides some practice cleaning variables which have common problems.\n",
    "1. Numeric variable: For `airbnb_NYC.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
    "\n",
    "To clean the 'Price' variable, I first converted everything into a string and removed formatting issues like dollar signs and commas. A value like 1,001 would have the comma removed. I also stripped extra spaces and treated blank or invalid entries as missing. After cleaning, I converted the values to numeric, changing any non-convertible values to become NaN. We end with 0 missing values. \n",
    "\n",
    "2. Categorical variable: For the Minnesota police use of for data, `mn_police_use_of_force.csv`, clean the `subject_injury` variable, handling the NA's; this gives a value `Yes` when a person was injured by police, and `No` when no injury occurred. What proportion of the values are missing? Cross-tabulate your cleaned `subject_injury` variable with the `force_type` variable. Are there any patterns regarding when the data are missing? For the remaining missing values, replace the `np.nan/None` values with the label `Missing`.\n",
    "\n",
    "For subject_injury, I standardized all text by trimming whitespace and converting it to lowercase, then grouped responses into Yes or No categories based on keywords such as “injured” or “no injury”. I treated ambiguous responses like “unknown” or “n/a” as missing values. Also, I calculated the proportion of missing values and examined how missingness varies across force_type using a cross-tab and a missing-rate summary. After analyzing the pattern, I replaced the remaining missing values with the label “Missing” so they could still be included in summaries.\n",
    "\n",
    "3. Dummy variable: For `metabric.csv`, convert the `Overall Survival Status` variable into a dummy/binary variable, taking the value 0 if the patient is deceased and 1 if they are living.\n",
    "\n",
    "I created a binary survival variable from Overall Survival Status by searching for keywords that indicate whether a patient was alive or dead. If the status contained terms like “living” or “alive” I assigned a 1 while if it contained terms like “deceased” “dead” I assigned a 0. Any entries that did not fall into one of these categories were left as missing to avoid misclassification.\n",
    "\n",
    "4. Missing values: For `airbnb_NYC.csv`, determine how many missing values of `Review Scores Rating` there are. Create a new variable, in which you impute the median score for non-missing observations to the missing ones. Why might this bias or otherwise negatively impact your results?\n",
    "\n",
    "I counted the number of missing values in Review Scores Rating and then I created a new variable where missing ratings were filled with the median of the observed ratings. Using the median helps reduce the impact of extreme values, but this method can still introduce somes biases. It reduces variation in the data and assumes that missing ratings are similar to the typical listing, which may not be true if missingness is not random and has a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8e6fa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1.1 Missing after cleaning: 0\n",
      "Q1.2 Missing proportion: 0.7619342359767892\n",
      "\n",
      "Q1.2 Crosstab (before labeling Missing):\n",
      "injury_clean                   No   Yes   NaN\n",
      "force_type                                   \n",
      "Baton                           0     2     2\n",
      "Bodily Force                 1093  1286  7051\n",
      "Chemical Irritant             131    41  1421\n",
      "Firearm                         2     0     0\n",
      "Gun Point Display              33    44    27\n",
      "Improvised Weapon              34    40    74\n",
      "Less Lethal                     0     0    87\n",
      "Less Lethal Projectile          1     2     0\n",
      "Maximal Restraint Technique     0     0   170\n",
      "Police K9 Bite                  2    44    31\n",
      "Taser                         150   172   985\n",
      "\n",
      "Missing rate by force_type:\n",
      "force_type\n",
      "Less Lethal                    1.000000\n",
      "Maximal Restraint Technique    1.000000\n",
      "Chemical Irritant              0.892028\n",
      "Taser                          0.753634\n",
      "Bodily Force                   0.747720\n",
      "Baton                          0.500000\n",
      "Improvised Weapon              0.500000\n",
      "Police K9 Bite                 0.402597\n",
      "Gun Point Display              0.259615\n",
      "Firearm                        0.000000\n",
      "Less Lethal Projectile         0.000000\n",
      "Name: inj_missing_flag, dtype: float64\n",
      "\n",
      "Crosstab after labeling Missing:\n",
      "injury_clean                 Missing    No   Yes    All\n",
      "force_type                                             \n",
      "Baton                              2     0     2      4\n",
      "Bodily Force                    7051  1093  1286   9430\n",
      "Chemical Irritant               1421   131    41   1593\n",
      "Firearm                            0     2     0      2\n",
      "Gun Point Display                 27    33    44    104\n",
      "Improvised Weapon                 74    34    40    148\n",
      "Less Lethal                       87     0     0     87\n",
      "Less Lethal Projectile             0     1     2      3\n",
      "Maximal Restraint Technique      170     0     0    170\n",
      "Police K9 Bite                    31     2    44     77\n",
      "Taser                            985   150   172   1307\n",
      "All                             9848  1446  1631  12925\n",
      "\n",
      "Q1.3 Dummy counts:\n",
      "survival_binary\n",
      "0    768\n",
      "1    575\n",
      "Name: count, dtype: int64\n",
      "Missing in dummy: 0\n",
      "\n",
      "Q1.4 Missing review scores: 8323\n",
      "Median used: 94.0\n",
      "Missing after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "airbnb_df = pd.read_csv(\"/Users/timothylee/DS Analytics/ml_container/data/airbnb_NYC.csv\", encoding=\"latin1\")\n",
    "police_df = pd.read_csv(\"/Users/timothylee/DS Analytics/ml_container/data/mn_police_use_of_force.csv\", encoding=\"latin1\")\n",
    "metabric_df = pd.read_csv(\"/Users/timothylee/DS Analytics/ml_container/data/metabric.csv\", encoding=\"latin1\")\n",
    "\n",
    "price_raw = airbnb_df[\"Price\"].astype(str).str.strip()\n",
    "\n",
    "price_cleaned = (\n",
    "    price_raw\n",
    "    .replace({\"\": np.nan, \"nan\": np.nan, \"None\": np.nan})\n",
    "    .str.replace(r\"[\\$,]\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "airbnb_df[\"price_numeric\"] = pd.to_numeric(price_cleaned, errors=\"coerce\")\n",
    "\n",
    "print(\"Q1.1 Missing after cleaning:\", airbnb_df[\"price_numeric\"].isna().sum())\n",
    "\n",
    "inj_raw = police_df[\"subject_injury\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "missing_vals = {\n",
    "    \"\", \"nan\", \"none\", \"null\", \"na\",\n",
    "    \"n/a\", \"unknown\", \"unsure\", \"undetermined\"\n",
    "}\n",
    "\n",
    "inj_raw = inj_raw.where(~inj_raw.isin(missing_vals), np.nan)\n",
    "\n",
    "yes_values = {\"yes\", \"y\", \"injured\", \"injury\", \"true\", \"1\"}\n",
    "no_values  = {\"no\", \"n\", \"not injured\", \"no injury\", \"false\", \"0\"}\n",
    "\n",
    "def recode_injury(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    if val in yes_values:\n",
    "        return \"Yes\"\n",
    "    if val in no_values:\n",
    "        return \"No\"\n",
    "    return np.nan\n",
    "\n",
    "police_df[\"injury_clean\"] = inj_raw.map(recode_injury)\n",
    "\n",
    "print(\"Q1.2 Missing proportion:\", police_df[\"injury_clean\"].isna().mean())\n",
    "\n",
    "print(\"\\nQ1.2 Crosstab (before labeling Missing):\")\n",
    "print(pd.crosstab(police_df[\"force_type\"],\n",
    "                  police_df[\"injury_clean\"],\n",
    "                  dropna=False))\n",
    "\n",
    "police_df[\"inj_missing_flag\"] = police_df[\"injury_clean\"].isna()\n",
    "\n",
    "print(\"\\nMissing rate by force_type:\")\n",
    "print(police_df.groupby(\"force_type\")[\"inj_missing_flag\"]\n",
    "      .mean()\n",
    "      .sort_values(ascending=False))\n",
    "\n",
    "police_df[\"injury_clean\"] = police_df[\"injury_clean\"].fillna(\"Missing\")\n",
    "\n",
    "print(\"\\nCrosstab after labeling Missing:\")\n",
    "print(pd.crosstab(police_df[\"force_type\"],\n",
    "                  police_df[\"injury_clean\"],\n",
    "                  margins=True))\n",
    "\n",
    "surv_raw = metabric_df[\"Overall Survival Status\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "def make_binary(status):\n",
    "    if pd.isna(status) or status in {\"\", \"nan\", \"none\", \"na\", \"n/a\"}:\n",
    "        return np.nan\n",
    "    if \"deceased\" in status or \"dead\" in status or \"died\" in status:\n",
    "        return 0\n",
    "    if \"living\" in status or \"alive\" in status:\n",
    "        return 1\n",
    "    if status in {\"0\", \"1\"}:\n",
    "        return int(status)\n",
    "    return np.nan\n",
    "\n",
    "metabric_df[\"survival_binary\"] = surv_raw.map(make_binary)\n",
    "\n",
    "print(\"\\nQ1.3 Dummy counts:\")\n",
    "print(metabric_df[\"survival_binary\"].value_counts(dropna=False))\n",
    "print(\"Missing in dummy:\", metabric_df[\"survival_binary\"].isna().sum())\n",
    "\n",
    "airbnb_df[\"Review Scores Rating\"] = pd.to_numeric(\n",
    "    airbnb_df[\"Review Scores Rating\"],\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "print(\"\\nQ1.4 Missing review scores:\",\n",
    "      airbnb_df[\"Review Scores Rating\"].isna().sum())\n",
    "\n",
    "median_score = airbnb_df[\"Review Scores Rating\"].median(skipna=True)\n",
    "\n",
    "airbnb_df[\"review_imputed\"] = (\n",
    "    airbnb_df[\"Review Scores Rating\"]\n",
    "    .fillna(median_score)\n",
    ")\n",
    "\n",
    "print(\"Median used:\", median_score)\n",
    "print(\"Missing after imputation:\",\n",
    "      airbnb_df[\"review_imputed\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a66bd",
   "metadata": {},
   "source": [
    "**Q2.** Go to https://sharkattackfile.net/ and download their dataset on shark attacks.\n",
    "\n",
    "1. Open the shark attack file using Pandas. It is probably not a csv file, so `read_csv` won't work. What does work?\n",
    "2. Drop any columns that do not contain data.\n",
    "3. What is an observation? Carefully justify your answer, and explain how it affects your choices in cleaning and analyzing the data.\n",
    "4. Clean the year variable. Describe the range of values you see. Filter the rows to focus on attacks since 1940. Are attacks increasing, decreasing, or remaining constant over time?\n",
    "5. Clean the Age variable and make a histogram of the ages of the victims.\n",
    "6. Clean the `Type` variable so it only takes three values: Provoked and Unprovoked and Unknown. What proportion of attacks are unprovoked?\n",
    "7. Clean the `Fatal Y/N` variable so it only takes three values: Y, N, and Unknown.\n",
    "8. Is the attack more or less likely to be fatal when the attack is provoked or unprovoked? Thoughts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f0744",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'xlrd'. Install xlrd >= 2.0.1 for xls Excel support Use pip or conda to install xlrd.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/compat/_optional.py:135\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1140\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'xlrd'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m shark_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGSAF5.xls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mxlrd\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m shark_df = shark_df.dropna(axis=\u001b[32m1\u001b[39m, how=\u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m year_text = shark_df[\u001b[33m\"\u001b[39m\u001b[33mYear\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/io/excel/_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/io/excel/_base.py:1567\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28mself\u001b[39m.engine = engine\n\u001b[32m   1565\u001b[39m \u001b[38;5;28mself\u001b[39m.storage_options = storage_options\n\u001b[32m-> \u001b[39m\u001b[32m1567\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1568\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_io\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/io/excel/_xlrd.py:45\u001b[39m, in \u001b[36mXlrdReader.__init__\u001b[39m\u001b[34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03mReader using xlrd engine.\u001b[39;00m\n\u001b[32m     35\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m \u001b[33;03m    Arbitrary keyword arguments passed to excel engine.\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     44\u001b[39m err_msg = \u001b[33m\"\u001b[39m\u001b[33mInstall xlrd >= 2.0.1 for xls Excel support\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mxlrd\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m=\u001b[49m\u001b[43merr_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     47\u001b[39m     filepath_or_buffer,\n\u001b[32m     48\u001b[39m     storage_options=storage_options,\n\u001b[32m     49\u001b[39m     engine_kwargs=engine_kwargs,\n\u001b[32m     50\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/compat/_optional.py:138\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Missing optional dependency 'xlrd'. Install xlrd >= 2.0.1 for xls Excel support Use pip or conda to install xlrd."
     ]
    }
   ],
   "source": [
    "shark_df = pd.read_excel(\"GSAF5.xls\", engine=\"xlrd\")\n",
    "shark_df = shark_df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "year_text = shark_df[\"Year\"].astype(str)\n",
    "year_digits = year_text.str.extract(r\"(\\d{4})\", expand=False)\n",
    "shark_df[\"year_num\"] = pd.to_numeric(year_digits, errors=\"coerce\")\n",
    "\n",
    "print(\"Cleaned year range:\",\n",
    "      shark_df[\"year_num\"].min(skipna=True),\n",
    "      \"to\",\n",
    "      shark_df[\"year_num\"].max(skipna=True))\n",
    "\n",
    "recent_df = shark_df[shark_df[\"year_num\"] >= 1940].copy()\n",
    "\n",
    "year_counts = (\n",
    "    recent_df\n",
    "    .groupby(\"year_num\")\n",
    "    .size()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "x_vals = year_counts.index.to_numpy()\n",
    "y_vals = year_counts.values\n",
    "\n",
    "trend_coef = np.polyfit(x_vals, y_vals, 1)[0] if len(x_vals) > 1 else np.nan\n",
    "print(\"Trend slope (attacks per year):\", trend_coef)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_vals, y_vals)\n",
    "plt.title(\"Shark Attacks per Year (1940+)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Attacks\")\n",
    "plt.show()\n",
    "\n",
    "age_raw = (\n",
    "    recent_df[\"Age\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .replace({\"\": np.nan, \"nan\": np.nan, \"none\": np.nan})\n",
    ")\n",
    "\n",
    "def process_age(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "\n",
    "    val = str(val).strip().lower()\n",
    "\n",
    "    match_range = pd.Series([val]).str.extract(r\"^(\\d+)\\s*-\\s*(\\d+)$\")\n",
    "    if not match_range.isna().all(axis=None):\n",
    "        low = float(match_range.iloc[0, 0])\n",
    "        high = float(match_range.iloc[0, 1])\n",
    "        return (low + high) / 2\n",
    "\n",
    "    match_single = pd.Series([val]).str.extract(r\"^(\\d+)$\")\n",
    "    if not match_single.isna().all(axis=None):\n",
    "        return float(match_single.iloc[0, 0])\n",
    "\n",
    "    match_any = pd.Series([val]).str.extract(r\"(\\d+)\")\n",
    "    if not match_any.isna().all(axis=None):\n",
    "        return float(match_any.iloc[0, 0])\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "recent_df[\"age_num\"] = age_raw.map(process_age)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(recent_df[\"age_num\"].dropna(), bins=30)\n",
    "plt.title(\"Victim Age Distribution (1940+)\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "type_raw = (\n",
    "    recent_df[\"Type\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .replace({\"\": np.nan, \"nan\": np.nan, \"none\": np.nan})\n",
    ")\n",
    "\n",
    "def categorize_type(entry):\n",
    "    if pd.isna(entry):\n",
    "        return \"Unknown\"\n",
    "\n",
    "    entry = str(entry).lower()\n",
    "\n",
    "    if \"unprovoked\" in entry:\n",
    "        return \"Unprovoked\"\n",
    "    if \"provoked\" in entry:\n",
    "        return \"Provoked\"\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "recent_df[\"type_group\"] = type_raw.map(categorize_type)\n",
    "\n",
    "prop_unprovoked = (recent_df[\"type_group\"] == \"Unprovoked\").mean()\n",
    "print(\"Share of unprovoked attacks (since 1940):\", prop_unprovoked)\n",
    "\n",
    "fatal_raw = (\n",
    "    recent_df[\"Fatal Y/N\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    "    .replace({\"\": np.nan, \"NAN\": np.nan, \"NONE\": np.nan})\n",
    ")\n",
    "\n",
    "def standardize_fatal(flag):\n",
    "    if pd.isna(flag):\n",
    "        return \"Unknown\"\n",
    "\n",
    "    flag = str(flag).upper()\n",
    "\n",
    "    if flag.startswith(\"Y\"):\n",
    "        return \"Y\"\n",
    "    if flag.startswith(\"N\"):\n",
    "        return \"N\"\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "recent_df[\"fatal_flag\"] = fatal_raw.map(standardize_fatal)\n",
    "\n",
    "print(pd.crosstab(recent_df[\"type_group\"],\n",
    "                  recent_df[\"fatal_flag\"],\n",
    "                  margins=True))\n",
    "\n",
    "fatal_rates = (\n",
    "    recent_df\n",
    "    .assign(fatal_binary=recent_df[\"fatal_flag\"] == \"Y\")\n",
    "    .groupby(\"type_group\")[\"fatal_binary\"]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "print(\"\\nFatality rate by type:\")\n",
    "print(fatal_rates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54a370",
   "metadata": {},
   "source": [
    "**Q3.** Open the \"tidy_data.pdf\" document available in `https://github.com/ds4e/wrangling`, which is a paper called *Tidy Data* by Hadley Wickham.\n",
    "\n",
    "  1. Read the abstract. What is this paper about?\n",
    "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
    "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, it’s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
    "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
    "  5. How is \"Tidy Data\" defined in section 2.3?\n",
    "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
    "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58495aa",
   "metadata": {},
   "source": [
    "**Q4.** This question looks at financial transfers from international actors to American universities. In particular, from which countries and giftors are the gifts coming from, and to which institutions are they going? \n",
    "\n",
    "For this question, `.groupby([vars]).count()` and `.groupby([vars]).sum()` will be especially useful to tally the number of occurrences and sum the values of those occurrences.\n",
    "\n",
    "1. Load the `ForeignGifts_edu.csv` dataset.\n",
    "2. For `Foreign Gift Amount`, create a histogram and describe the variable. Describe your findings.\n",
    "3. For `Gift Type`, create a histogram or value counts table. What proportion of the gifts are contracts, real estate, and monetary gifts?\n",
    "4. What are the top 15 countries in terms of the number of gifts? What are the top 15 countries in terms of the amount given?\n",
    "5. What are the top 15 institutions in terms of the total amount of money they receive? Make a histogram of the total amount received by all institutions. \n",
    "6. Which giftors provide the most money, in total? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a8e97",
   "metadata": {},
   "source": [
    "**Q5.** This question provides some practice doing exploratory data analysis and visualization.\n",
    "\n",
    "We'll use the `college_completion.csv` dataset from the US Department of Education. The \"relevant\" variables for this question are:\n",
    "  - `level` - Level of institution (4-year, 2-year)\n",
    "  - `aid_value` - The average amount of student aid going to undergraduate recipients\n",
    "  - `control` - Public, Private not-for-profit, Private for-profit\n",
    "  - `grad_100_value` - percentage of first-time, full-time, degree-seeking undergraduates who complete a degree or certificate program within 100 percent of expected time (bachelor's-seeking group at 4-year institutions)\n",
    "\n",
    "1. Load the `college_completion.csv` data with Pandas.\n",
    "2. How many observations and variables are in the data? Use `.head()` to examine the first few rows of data.\n",
    "3. Cross tabulate `control` and `level`. Describe the patterns you see in words.\n",
    "4. For `grad_100_value`, create a kernel density plot and describe table. Now condition on `control`, and produce a kernel density plot and describe tables for each type of institutional control. Which type of institution appear to have the most favorable graduation rates?\n",
    "5. Make a scatterplot of `grad_100_value` by `aid_value`, and compute the covariance and correlation between the two variables. Describe what you see. Now make the same plot and statistics, but conditioning on `control`. Describe what you see. For which kinds of institutions does aid seem to vary positively with graduation rates?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1a468b",
   "metadata": {},
   "source": [
    "**Q6.** In class, we talked about how to compute the sample mean of a variable $X$,\n",
    "$$\n",
    "m(X) = \\dfrac{1}{N} \\sum_{i=1}^N x_i\n",
    "$$\n",
    "and sample covariance of two variables $X$ and $Y$,\n",
    "$$\n",
    "\\text{cov}(X,Y) = \\dfrac{1}{N} \\sum_{i=1}^N (x_i - m(X))(y_i - m(Y))).\n",
    "$$\n",
    "Recall, the sample variance of $X$ is\n",
    "$$\n",
    "s^2 = \\dfrac{1}{N} \\sum_{i=1}^N (x_i - m(X))^2.\n",
    "$$\n",
    "It can be very helpful to understand some basic properties of these statistics. If you want to write your calculations on a piece of paper, take a photo, and upload that to your GitHub repo, that's probably easiest.\n",
    "\n",
    "We're going to look at **linear transformations** of $X$, $Y = a + bX$. So we take each value of $X$, $x_i$, and transform it as $y_i = a + b x_i$. \n",
    "\n",
    "1. Show that $m(a + bX) = a+b \\times m(X)$.\n",
    "2. Show that $ \\text{cov}(X,X) = s^2$.\n",
    "3. Show that $\\text{cov}(X,a+bY) = b \\times \\text{cov}(X,Y)$\n",
    "4. Show that $\\text{cov}(a+bX,a+bY) = b^2 \\text{cov}(X,Y) $. Notice, this also means that $\\text{cov}(bX, bX) = b^2 s^2$.\n",
    "5. Suppose $b>0$ and let the median of $X$ be $\\text{med}(X)$. Is it true that the median of $a+bX$ is equal to $a + b \\times \\text{med}(X)$? Is the IQR of $a + bX$ equal to $a + b \\times \\text{IQR}(X)$?\n",
    "6. Show by example that the means of $X^2$ and $\\sqrt{X}$ are generally not $(m(X))^2$ and $\\sqrt{m(X)}$. So, the results we derived above really depend on the linearity of the transformation $Y = a + bX$, and transformations like $Y = X^2$ or $Y = \\sqrt{X}$ will not behave in a similar way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f54b39",
   "metadata": {},
   "source": [
    "**Q7.** This question provides some practice doing exploratory data analysis and visualization.\n",
    "\n",
    "We'll use the `ames_prices.csv` dataset. The \"relevant\" variables for this question are:\n",
    "  - `price` - Sale price value of the house\n",
    "  - `Bldg.Type` - Building type of the house (single family home, end-of-unit townhome, duplex, interior townhome, two-family conversion)\n",
    "\n",
    "1. Load the `college_completion.csv` data with Pandas.\n",
    "2. Make a kernel density plot of price and compute a describe table. Now, make a kernel density plot of price conditional on building type, and use `.groupby()` to make a describe type for each type of building. Which building types are the most expensive, on average? Which have the highest variance in transaction prices?\n",
    "3. Make an ECDF plot of price, and compute the sample minimum, .25 quantile, median, .75 quantile, and sample maximum (i.e. a 5-number summary).\n",
    "4. Make a boxplot of price. Are there outliers? Make a boxplot of price conditional on building type. What patterns do you see?\n",
    "5. Make a dummy variable indicating that an observation is an outlier.\n",
    "6. Winsorize the price variable, and compute a new kernel density plot and describe table. How do the results change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
